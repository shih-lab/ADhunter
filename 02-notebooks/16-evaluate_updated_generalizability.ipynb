{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n"
     ]
    }
   ],
   "source": [
    "from main import *\n",
    "\n",
    "NOTEBOOK_ID = '16'\n",
    "MAKE_DISTANCE_MATRIX = False\n",
    "RUN_CLUSTER_SCAN = False\n",
    "RUN_CLUSTERING = False\n",
    "TRAIN_MODEL = False\n",
    "TRAIN_ENSEMBLE = False\n",
    "RUN_DIAMOND = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# determine number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m     distance_df\u001b[38;5;241m.\u001b[39mto_pickle(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mGENERALIZABILITY_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/01-dataset/harmonized_distance_matrix.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \n\u001b[0;32m---> 14\u001b[0m     distance_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_pickle\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mGENERALIZABILITY_DIR\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/01-dataset/harmonized_distance_matrix.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m distance_df\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RUN_CLUSTER_SCAN:\n",
      "File \u001b[0;32m~/anaconda3/envs/adhunter/lib/python3.8/site-packages/pandas/io/pickle.py:196\u001b[0m, in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m warnings\u001b[38;5;241m.\u001b[39mcatch_warnings(record\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    194\u001b[0m         \u001b[38;5;66;03m# We want to silence any warnings about, e.g. moved modules.\u001b[39;00m\n\u001b[1;32m    195\u001b[0m         warnings\u001b[38;5;241m.\u001b[39msimplefilter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mWarning\u001b[39;00m)\n\u001b[0;32m--> 196\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandles\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m excs_to_catch:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;66;03m# e.g.\u001b[39;00m\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;66;03m#  \"No module named 'pandas.core.sparse.series'\"\u001b[39;00m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;66;03m#  \"Can't get attribute '__nat_unpickle' on <module 'pandas._libs.tslib\"\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pc\u001b[38;5;241m.\u001b[39mload(handles\u001b[38;5;241m.\u001b[39mhandle, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/adhunter/lib/python3.8/site-packages/numpy/core/numeric.py:1874\u001b[0m, in \u001b[0;36m_frombuffer\u001b[0;34m(buf, dtype, shape, order)\u001b[0m\n\u001b[1;32m   1866\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m function(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1869\u001b[0m _fromfunction_with_like \u001b[38;5;241m=\u001b[39m array_function_dispatch(\n\u001b[1;32m   1870\u001b[0m     _fromfunction_dispatcher, use_like\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1871\u001b[0m )(fromfunction)\n\u001b[0;32m-> 1874\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_frombuffer\u001b[39m(buf, dtype, shape, order):\n\u001b[1;32m   1875\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m frombuffer(buf, dtype\u001b[38;5;241m=\u001b[39mdtype)\u001b[38;5;241m.\u001b[39mreshape(shape, order\u001b[38;5;241m=\u001b[39morder)\n\u001b[1;32m   1878\u001b[0m \u001b[38;5;129m@set_module\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1879\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21misscalar\u001b[39m(element):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if MAKE_DISTANCE_MATRIX:\n",
    "    harmonized_df = pd.read_pickle(f'{HARMONIZE_DIR}/04-results/esm2_t33_650M_UR50D-layer33-representations.pkl').query('dataset != \"Mycocosm_Overlap\"').drop_duplicates(subset='AAseq',keep=False)\n",
    "    harmonized_df = pd.read_csv(f'{HARMONIZE_DIR}/04-results/new_to_old_harmonized_dataset.csv')\n",
    "    harmonized_df = harmonized_df[['dataset','AAseq','linear_harmonized_activity_scaled','linear_harmonized_activity','polynomial_harmonized_activity_scaled','polynomial_harmonized_activity','sigmoid_harmonized_activity_scaled','sigmoid_harmonized_activity']]\n",
    "    harmonized_df['id'] = [f'harmonized-{str(i).zfill(5)}' for i in range(len(harmonized_df))]\n",
    "    df_to_fasta(harmonized_df,'ID','AAseq',f'{GENERALIZABILITY_DIR}/01-dataset/harmonized_dedup.faa')\n",
    "    os.system(f'clustalo -i harmonized_dedup.faa --distmat-out={GENERALIZABILITY_DIR}/01-dataset/harmonized_distance_matrix.txt --full --percent-id --output-order=tree-order --threads=10 --force')\n",
    "\n",
    "if not os.path.isfile(f'{GENERALIZABILITY_DIR}/01-dataset/harmonized_distance_matrix.pkl'):\n",
    "    distance_df = pd.read_table(f'{GENERALIZABILITY_DIR}/01-dataset/harmonized_distance_matrix.txt',skiprows=1,sep=' ',index_col=0,header=None)\n",
    "    distance_df.columns = distance_df.index\n",
    "    distance_df.to_pickle(f'{GENERALIZABILITY_DIR}/01-dataset/harmonized_distance_matrix.pkl')\n",
    "else: \n",
    "    distance_df = pd.read_pickle(f'{GENERALIZABILITY_DIR}/01-dataset/harmonized_distance_matrix.pkl')\n",
    "distance_df.index.name = None\n",
    "\n",
    "if RUN_CLUSTER_SCAN:\n",
    "    max_clusters = 5\n",
    "    silhouette_scores = calculate_silhouette(distance_df.to_numpy(), max_clusters)\n",
    "\n",
    "    plt.figure(figsize=(4,3))\n",
    "    g = sns.lineplot(x=range(2, len(silhouette_scores) + 2), y=silhouette_scores,linewidth=3)\n",
    "    plt.xticks(np.arange(2, int(6), 1))\n",
    "    g.set(title=\"Silhouette Score vs Number of Clusters\",xlabel=\"Number of Clusters\",ylabel=\"Silhouette Score\")\n",
    "    if SAVE_FIGURES:\n",
    "        plt.savefig(f'{FIGURE_DIR}/15-silhouette_score.png',dpi=400,bbox_inches='tight',transparent=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run spectral clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_CLUSTERING:\n",
    "    harmonized_df = pd.read_pickle(f'{HARMONIZE_DIR}/04-results/esm2_t33_650M_UR50D-layer33-representations.pkl').query('dataset != \"Mycocosm_Overlap\"').drop_duplicates(subset='AAseq',keep=False)\n",
    "    clustering = SpectralClustering(n_clusters=3, affinity='precomputed')\n",
    "    labels = clustering.fit_predict(distance_df.to_numpy())\n",
    "    cluster_df = harmonized_df[['ID','AAseq','linear_harmonized_activity','dataset']].copy()\n",
    "    cluster_df['cluster'] = labels+1\n",
    "    cluster_df.to_pickle(f'{GENERALIZABILITY_DIR}/01-dataset/cluster_df.pkl')\n",
    "else:\n",
    "    cluster_df = pd.read_pickle(f'{GENERALIZABILITY_DIR}/01-dataset/cluster_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,3))\n",
    "tab10 = sns.color_palette('tab10')\n",
    "g = sns.histplot(data=cluster_df.query('cluster == 1'),x='linear_harmonized_activity',element='step',color=tab10[0],label='1',fill=False)\n",
    "sns.histplot(data=cluster_df.query('cluster == 2'),x='linear_harmonized_activity',element='step',color=tab10[1],label='2',fill=False)\n",
    "# sns.histplot(data=cluster_df.query('cluster == 3'),x='linear_harmonized_activity',element='step',color=tab10[2],label='3',fill=False)\n",
    "\n",
    "g.legend(title='Cluster',frameon=False)\n",
    "plt.savefig(f'{FIGURE_DIR}/15-histplot_spectral_clustering.png',dpi=400,bbox_inches='tight',transparent=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train/test ADhunter v1 \n",
    "* train on largest cluster\n",
    "* validate on second largest cluster\n",
    "* test on third largest cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_MODEL:\n",
    "        cluster_df['linear_harmonized_activity_standardscaled'] = preprocessing.StandardScaler().fit_transform( cluster_df['linear_harmonized_activity'].to_numpy().reshape(-1,1) )\n",
    "        y = cluster_df['linear_harmonized_activity_standardscaled'].to_numpy()\n",
    "        threshold = cluster_df['linear_harmonized_activity_standardscaled'].median()\n",
    "        y_cont = y.reshape(-1, 1)\n",
    "        y_bin = (y >= threshold).astype(np.int64).reshape(-1, 1)\n",
    "        for state in range(1,11):\n",
    "            train_df = cluster_df.query(f'cluster == 1')\n",
    "            X_train = np.asarray([[aa_to_i[aa] for aa in x] for x in train_df['AAseq']])\n",
    "            y_train = train_df['linear_harmonized_activity_standardscaled'].to_numpy().reshape(-1,1)\n",
    "            y_train_bin= (y_train >= threshold).astype(np.int64).reshape(-1, 1)\n",
    "            splitter = StratifiedShuffleSplit(n_splits=1,random_state=state)\n",
    "            train_idx = np.concatenate(list(splitter.split(X_train, y_train_bin))[0])\n",
    "            X_train = torch.tensor(X_train[train_idx])\n",
    "            y_train = torch.tensor(y_train[train_idx])\n",
    "\n",
    "            val_df = cluster_df.query(f'cluster == 3')\n",
    "            X_val = np.asarray([[aa_to_i[aa] for aa in x] for x in val_df['AAseq']])\n",
    "            y_val = val_df['linear_harmonized_activity_standardscaled'].to_numpy().reshape(-1,1)\n",
    "            y_val_bin = (y_val >= threshold).astype(np.int64).reshape(-1, 1)\n",
    "            splitter = StratifiedShuffleSplit(n_splits=1,random_state=state)\n",
    "            val_idx = np.concatenate(list(splitter.split(X_val, y_val_bin))[0])\n",
    "            X_val = torch.tensor(X_val[val_idx])\n",
    "            y_val = torch.tensor(y_val[val_idx])\n",
    "\n",
    "            test_df = cluster_df.query(f'cluster == 2')\n",
    "            X_test = np.asarray([[aa_to_i[aa] for aa in x] for x in test_df['AAseq']])\n",
    "            y_test = test_df['linear_harmonized_activity_standardscaled'].to_numpy().reshape(-1,1)\n",
    "            y_test_bin = (y_test >= threshold).astype(np.int64).reshape(-1, 1)\n",
    "            splitter = StratifiedShuffleSplit(n_splits=1,random_state=state)\n",
    "            test_idx = np.concatenate(list(splitter.split(X_test, y_test_bin))[0])\n",
    "            X_test = torch.tensor(X_test[test_idx])\n",
    "            y_test = torch.tensor(y_test[test_idx])\n",
    "            y_test_bin = torch.tensor(y_test_bin)\n",
    "            dataset = (X_train, None, y_train), (X_val, _, y_val), (X_test, y_test_bin, y_test)\n",
    "            train_ADhunter(dataset,out_dir=f'{GENERALIZABILITY_DIR}/02-evaluate',out_name=f'spectral_cluster',version='v1',random_state=state)\n",
    "            test_ADhunter(dataset,out_dir=f'{GENERALIZABILITY_DIR}/02-evaluate',out_name=f'spectral_cluster',version='v1',random_state=state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train/test ADhunter v2\n",
    "* train on largest cluster\n",
    "* validate on second largest cluster\n",
    "* test on third largest cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_MODEL:\n",
    "    harmonized_df = pd.read_pickle(f'{HARMONIZE_DIR}/04-results/esm2_t33_650M_UR50D-layer33-representations.pkl').query('dataset != \"Mycocosm_Overlap\"').drop_duplicates(subset='AAseq',keep=False)\n",
    "    cluster_df['linear_harmonized_activity_standardscaled'] = preprocessing.StandardScaler().fit_transform( cluster_df['linear_harmonized_activity'].to_numpy().reshape(-1,1) )\n",
    "    harmonized_df = harmonized_df.drop(columns=['linear_harmonized_activity']).merge(cluster_df[['AAseq','linear_harmonized_activity_standardscaled','linear_harmonized_activity','cluster']],on='AAseq')\n",
    "    y = harmonized_df['linear_harmonized_activity_standardscaled'].to_numpy()\n",
    "    threshold = harmonized_df['linear_harmonized_activity_standardscaled'].median()\n",
    "    y_cont = y.reshape(-1, 1)\n",
    "    y_bin = (y >= threshold).astype(np.int64).reshape(-1, 1)\n",
    "    for state in range(1,11):\n",
    "        train_df = harmonized_df.query(f'cluster == 1')\n",
    "        X_train = np.asarray([np.array(emb) for emb in train_df['esm2_t33_650M_UR50D']])\n",
    "        y_train = train_df['linear_harmonized_activity_standardscaled'].to_numpy().reshape(-1,1)\n",
    "        y_train_bin= (y_train >= threshold).astype(np.int64).reshape(-1, 1)\n",
    "        splitter = StratifiedShuffleSplit(n_splits=1,random_state=state)\n",
    "        train_idx = np.concatenate(list(splitter.split(X_train, y_train_bin))[0])\n",
    "        X_train = torch.tensor(X_train[train_idx])\n",
    "        y_train = torch.tensor(y_train[train_idx])\n",
    "\n",
    "        val_df = harmonized_df.query(f'cluster == 3')\n",
    "        X_val = np.asarray([np.array(emb) for emb in val_df['esm2_t33_650M_UR50D']])\n",
    "        y_val = val_df['linear_harmonized_activity_standardscaled'].to_numpy().reshape(-1,1)\n",
    "        y_val_bin = (y_val >= threshold).astype(np.int64).reshape(-1, 1)\n",
    "        splitter = StratifiedShuffleSplit(n_splits=1,random_state=state)\n",
    "        val_idx = np.concatenate(list(splitter.split(X_val, y_val_bin))[0])\n",
    "        X_val = torch.tensor(X_val[val_idx])\n",
    "        y_val = torch.tensor(y_val[val_idx])\n",
    "\n",
    "        test_df = harmonized_df.query(f'cluster == 2')\n",
    "        X_test = np.asarray([np.array(emb) for emb in test_df['esm2_t33_650M_UR50D']])\n",
    "        y_test = test_df['linear_harmonized_activity_standardscaled'].to_numpy().reshape(-1,1)\n",
    "        y_test_bin = (y_test >= threshold).astype(np.int64).reshape(-1, 1)\n",
    "        splitter = StratifiedShuffleSplit(n_splits=1,random_state=state)\n",
    "        test_idx = np.concatenate(list(splitter.split(X_test, y_test_bin))[0])\n",
    "        X_test = torch.tensor(X_test[test_idx])\n",
    "        y_test = torch.tensor(y_test[test_idx])\n",
    "        y_test_bin = torch.tensor(y_test_bin)\n",
    "\n",
    "        dataset = (X_train, None, y_train), (X_val, _, y_val), (X_test, y_test_bin, y_test)\n",
    "        train_ADhunter(dataset,out_dir=f'{GENERALIZABILITY_DIR}/02-evaluate',out_name=f'spectral_cluster',version='v2',random_state=state)\n",
    "        test_ADhunter(dataset,out_dir=f'{GENERALIZABILITY_DIR}/02-evaluate',out_name=f'spectral_cluster',version='v2',random_state=state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train/test ADhunter v2 ensemble\n",
    "* train on largest cluster\n",
    "* validate on second largest cluster\n",
    "* test on third largest cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_ENSEMBLE:\n",
    "    harmonized_df = pd.read_pickle(f'{HARMONIZE_DIR}/04-results/esm2_t33_650M_UR50D-layer33-representations.pkl').query('dataset != \"Mycocosm_Overlap\"').drop_duplicates('AAseq',keep=False)\n",
    "    cluster_df['linear_harmonized_activity_standardscaled'] = preprocessing.StandardScaler().fit_transform( cluster_df['linear_harmonized_activity'].to_numpy().reshape(-1,1) )\n",
    "    harmonized_df = harmonized_df.drop(columns=['linear_harmonized_activity']).merge(cluster_df[['AAseq','linear_harmonized_activity_standardscaled','linear_harmonized_activity','cluster']],on='AAseq')\n",
    "\n",
    "    y = harmonized_df['linear_harmonized_activity_standardscaled'].to_numpy()\n",
    "    threshold = harmonized_df['linear_harmonized_activity_standardscaled'].median()\n",
    "    y_cont = y.reshape(-1, 1)\n",
    "    y_bin = (y >= threshold).astype(np.int64).reshape(-1, 1)\n",
    "\n",
    "    for state in range(1,11):\n",
    "        train_df = harmonized_df.query(f'cluster == 1')\n",
    "        X_train = np.asarray([np.array(emb) for emb in train_df['esm2_t33_650M_UR50D']])\n",
    "        y_train = train_df['linear_harmonized_activity_standardscaled'].to_numpy().reshape(-1,1)\n",
    "        y_train_bin= (y_train >= threshold).astype(np.int64).reshape(-1, 1)\n",
    "        splitter = StratifiedShuffleSplit(n_splits=1,random_state=state)\n",
    "        train_idx = np.concatenate(list(splitter.split(X_train, y_train_bin))[0])\n",
    "        X_train = torch.tensor(X_train[train_idx])\n",
    "        y_train = torch.tensor(y_train[train_idx])\n",
    "\n",
    "        val_df = harmonized_df.query(f'cluster == 3')\n",
    "        X_val = np.asarray([np.array(emb) for emb in val_df['esm2_t33_650M_UR50D']])\n",
    "        y_val = val_df['linear_harmonized_activity_standardscaled'].to_numpy().reshape(-1,1)\n",
    "        y_val_bin = (y_val >= threshold).astype(np.int64).reshape(-1, 1)\n",
    "        splitter = StratifiedShuffleSplit(n_splits=1,random_state=state)\n",
    "        val_idx = np.concatenate(list(splitter.split(X_val, y_val_bin))[0])\n",
    "        X_val = torch.tensor(X_val[val_idx])\n",
    "        y_val = torch.tensor(y_val[val_idx])\n",
    "\n",
    "        test_df = harmonized_df.query(f'cluster == 2')\n",
    "        X_test = np.asarray([np.array(emb) for emb in test_df['esm2_t33_650M_UR50D']])\n",
    "        y_test = test_df['linear_harmonized_activity_standardscaled'].to_numpy().reshape(-1,1)\n",
    "        y_test_bin = (y_test >= threshold).astype(np.int64).reshape(-1, 1)\n",
    "        splitter = StratifiedShuffleSplit(n_splits=1,random_state=state)\n",
    "        test_idx = np.concatenate(list(splitter.split(X_test, y_test_bin))[0])\n",
    "        X_test = torch.tensor(X_test[test_idx])\n",
    "        y_test = torch.tensor(y_test[test_idx])\n",
    "        y_test_bin = torch.tensor(y_test_bin)\n",
    "\n",
    "        query_df = pd.read_csv(f'{ENSEMBLE_DIR}/03-hamming_subset/03-results/top_models.csv')\n",
    "        df_filtered = pd.read_csv(f'{ENSEMBLE_DIR}/03-hamming_subset/03-results/model_testing_filtered.csv').set_index('file')\n",
    "        # train models\n",
    "        out_dir = f'{GENERALIZABILITY_DIR}/02-evaluate'\n",
    "        _, selected_indices = select_maximally_different_arrays(df_filtered.to_numpy(), 20)\n",
    "        hits_df = df_filtered.reset_index()\n",
    "        hits_df = hits_df.loc[selected_indices].set_index('file')\n",
    "        for idx, row in query_df[query_df.file.isin(hits_df.index)].reset_index(drop=True).iterrows():\n",
    "            print(f'==================={idx/20}===================')\n",
    "            train_ds = TensorDataset(X_train.to(torch.float), y_train.to(torch.float))\n",
    "            val_ds = TensorDataset(X_val.to(torch.float), y_val.to(torch.float))\n",
    "            train_dl = DataLoader(train_ds, batch_size=row['batch_size'], shuffle=False)\n",
    "            val_dl = DataLoader(val_ds, batch_size=row['batch_size'], shuffle=False)\n",
    "\n",
    "            model = ADhunterSystem_v2(\n",
    "                embedding_size=X_train[0].shape[1],\n",
    "                hidden=row['hidden_size'], \n",
    "                kernel_size=row['kernel_size'], \n",
    "                dilation=row['dilation'], \n",
    "                num_res_blocks=row['num_res_blocks'],\n",
    "                seq_len=X_train[0].shape[0]\n",
    "                )\n",
    "\n",
    "            OUTPUT_FILE = f\"ADhunter_v2_h{row['hidden_size']}_k{row['kernel_size']}_d{row['dilation']}_r{row['num_res_blocks']}_b{row['batch_size']}_spectralclustering_state{state}\"\n",
    "            csv_logger = CSVLogger(f\"{out_dir}/01-logs\",name=OUTPUT_FILE,version='')\n",
    "            checkpoint_callback = ModelCheckpoint(dirpath=f\"{out_dir}/02-models\", monitor=\"val_loss\", filename=OUTPUT_FILE, save_last=False)\n",
    "            early_stopping = EarlyStopping('val_loss', patience=PATIENCE)\n",
    "            trainer = pl.Trainer(accelerator='gpu', devices=1, callbacks=[checkpoint_callback, early_stopping], logger=[csv_logger], max_epochs=MAX_EPOCHS)\n",
    "            trainer.fit(model, train_dataloaders=train_dl, val_dataloaders=val_dl)\n",
    "\n",
    "            model = model.load_from_checkpoint(checkpoint_callback.best_model_path)\n",
    "            torch.save(model.cpu().state_dict(), f'{out_dir}/02-models/{OUTPUT_FILE}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_df = pd.read_csv(f'{HAMMING_ENSEMBLE_DIR}/03-results/top_models.csv')\n",
    "df_filtered = pd.read_csv(f'{HAMMING_ENSEMBLE_DIR}/03-results/model_testing_filtered.csv').set_index('file')\n",
    "\n",
    "out_dir = f'{GENERALIZABILITY_DIR}/02-evaluate'\n",
    "_, selected_indices = select_maximally_different_arrays(df_filtered.to_numpy(), 20)\n",
    "hits_df = df_filtered.reset_index()\n",
    "hits_df = hits_df.loc[selected_indices].set_index('file')\n",
    "\n",
    "harmonized_df = pd.read_pickle(f'{HARMONIZE_DIR}/04-results/esm2_t33_650M_UR50D-layer33-representations.pkl').query('dataset != \"Mycocosm_Overlap\"').drop_duplicates('AAseq',keep=False)\n",
    "cluster_df['linear_harmonized_activity_standardscaled'] = preprocessing.StandardScaler().fit_transform( cluster_df['linear_harmonized_activity'].to_numpy().reshape(-1,1) )\n",
    "harmonized_df = harmonized_df.drop(columns=['linear_harmonized_activity']).merge(cluster_df[['AAseq','linear_harmonized_activity_standardscaled','linear_harmonized_activity','cluster']],on='AAseq')\n",
    "y = harmonized_df['linear_harmonized_activity_standardscaled'].to_numpy()\n",
    "threshold = harmonized_df['linear_harmonized_activity_standardscaled'].median()\n",
    "y_cont = y.reshape(-1, 1)\n",
    "y_bin = (y >= threshold).astype(np.int64).reshape(-1, 1)\n",
    "\n",
    "test_df = harmonized_df.query(f'cluster == 1')\n",
    "X_test = np.asarray([np.array(emb) for emb in test_df['esm2_t33_650M_UR50D']])\n",
    "y_test = test_df['linear_harmonized_activity_standardscaled'].to_numpy().reshape(-1,1)\n",
    "y_test_bin = (y_test >= threshold).astype(np.int64).reshape(-1, 1)\n",
    "splitter = StratifiedShuffleSplit(n_splits=1,random_state=0)\n",
    "test_idx = np.concatenate(list(splitter.split(X_test, y_test_bin))[0])\n",
    "X_test = torch.tensor(X_test[test_idx])\n",
    "y_test = torch.tensor(y_test[test_idx])\n",
    "y_test_bin = torch.tensor(y_test_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if not os.path.isfile(f'{GENERALIZABILITY_DIR}/ADhunter_spectral_clustering.pkl'):\n",
    "    test_df = None\n",
    "    for state in range(1,4):\n",
    "        new_predictions, new_uncertainty = get_new_uncertainty(X_test,state=state)\n",
    "        tmp_df = pd.DataFrame([new_predictions,y_test.numpy().flatten()]).T\n",
    "        tmp_df.columns=['y_test_hat','y_test']\n",
    "        tmp_df['uncertainty'] = new_uncertainty\n",
    "        tmp_df['params'] = 'ADhunter_ensemble'\n",
    "        tmp_df['state'] = state\n",
    "        test_df = pd.concat([test_df,tmp_df]).reset_index(drop=True)\n",
    "    test_df.to_pickle(f'{GENERALIZABILITY_DIR}/ADhunter_spectral_clustering.pkl')\n",
    "else:\n",
    "    test_df = pd.read_pickle(f'{GENERALIZABILITY_DIR}/ADhunter_spectral_clustering.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4820791562895386, 0.340602582589928, 0.44874058876525513)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr(test_df['y_test'], test_df['y_test_hat'])[0],spearmanr(test_df['y_test'], test_df['y_test_hat'])[0],mean_squared_error(test_df['y_test'], test_df['y_test_hat'], squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6464566929133858"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1.27-0.449)/1.27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 150x250 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tmp_df = pd.DataFrame([[1.27,'TADA'],[0.449,'ADhunter']])\n",
    "tmp_df.columns = ['RMSE','Model']\n",
    "fig = plt.figure(figsize=(1.5,2.5))\n",
    "g = sns.barplot(data=tmp_df,x='Model',y='RMSE',edgecolor='black',linewidth=0.5,color='#ceb9d9',alpha=1,saturation=1)\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{FIGURE_DIR}/{NOTEBOOK_ID}-spectral_rmse.svg',bbox_inches='tight',transparent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df = pd.read_pickle(f'{GENERALIZABILITY_DIR}/01-dataset/cluster_df.pkl')\n",
    "cluster_df['activity_scaled'] = preprocessing.StandardScaler().fit_transform( cluster_df['linear_harmonized_activity'].to_numpy().reshape(-1,1))\n",
    "y = cluster_df['activity_scaled'].to_numpy()\n",
    "threshold = cluster_df['activity_scaled'].median()\n",
    "cluster_df['activity_bin'] = cluster_df['activity_scaled'].apply(lambda x: x >= 1.0)\n",
    "cluster_df.to_pickle(f'{GENERALIZABILITY_DIR}/spectral_cluster_dataset.pkl')\n",
    "\n",
    "if RUN_DIAMOND:\n",
    "    df_to_fasta(cluster_df.query('cluster == 2'),'ID','AAseq',f'{GENERALIZABILITY_DIR}/test_cluster_seqs.faa')\n",
    "    os.system(f'diamond blastp -d {DATASET_DIR}/03-waldburger/mycocosm_db.dmnd \\\n",
    "    -q {GENERALIZABILITY_DIR}/test_cluster_seqs.faa -o {GENERALIZABILITY_DIR}/test_cluster_matches.tsv --id 100')\n",
    "\n",
    "cols = ['qseqid', 'sseqid', 'pident', 'length', 'mismatch', 'gapopen','qstart', 'qend', 'sstart', 'send', 'evalue', 'bitscore']\n",
    "matches_df = pd.read_csv(f'{GENERALIZABILITY_DIR}/test_cluster_matches.tsv',sep='\\t',names=cols)\n",
    "matches_df = matches_df.query('length == 53').reset_index(drop=True)\n",
    "matches_df['organism_id'] = matches_df['sseqid'].apply(lambda x: x.split('-')[0])\n",
    "matches_df = matches_df.merge(cluster_df,left_on='qseqid',right_on='ID') #4109 -> 4109\n",
    "matches_df = matches_df.drop(columns=['qseqid','qend','send','sseqid','pident','length','mismatch','gapopen','qstart','sstart','evalue','bitscore','linear_harmonized_activity','AAseq'])\n",
    "matches_df['dataset'] = matches_df['dataset'].replace({'Hummel_Overlap':'Hummel'})\n",
    "matches_df.to_pickle(f'{GENERALIZABILITY_DIR}/matches.pkl')\n",
    "\n",
    "count_df = (matches_df\n",
    "            .groupby(['organism_id'])['ID']\n",
    "            .count()\n",
    "            .reset_index()\n",
    "            .rename(columns={'ID':'count'}))\n",
    "count_df.to_csv(f'{GENERALIZABILITY_DIR}/test_counts.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adhunter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
