{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "\n",
    "NOTEBOOK_ID = '22'\n",
    "RUN_ESM = False\n",
    "SAVE_IDXS = False\n",
    "RUN_MODELS = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# encode new datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_ESM:\n",
    "    morffy_df = pd.read_csv(f'{COMPARE_MORFFY_DIR}/01-dataset/TrainingsData.csv').drop_duplicates(subset='Sequence',keep=False)\n",
    "    morffy_df = morffy_df[morffy_df['Sequence'].apply(lambda x: len(x) == 40)].rename(columns={'Sequence':'AAseq'})\n",
    "\n",
    "    model = 'esm2_t33_650M_UR50D'\n",
    "    layer = 33\n",
    "    tmp_df = morffy_df.copy()\n",
    "    out_faa = f'{COMPARE_MORFFY_DIR}/01-dataset/morffy.faa'\n",
    "    embeddings_dir = f'{COMPARE_MORFFY_DIR}/01-dataset/morffy_{model}'\n",
    "    out_pkl = f'{COMPARE_MORFFY_DIR}/01-dataset/morffy_{model}-layer{layer}-representations.pkl'\n",
    "\n",
    "    tmp_df['ID'] = [f'morffy-{str(i).zfill(6)}' for i in range(len(tmp_df))]\n",
    "    df_to_fasta(tmp_df,'ID','AAseq',out_faa)\n",
    "    if len(glob(f'{embeddings_dir}/*')) == 0:\n",
    "        print('running ESM')\n",
    "        run_esm(out_faa,\n",
    "                embeddings_dir,\n",
    "                model=model,\n",
    "                layer=layer)\n",
    "    if not os.path.isfile(out_pkl):\n",
    "        print('saving representations')\n",
    "        embedding_df = tmp_df.copy()\n",
    "        embedding_df[model] = embedding_df['ID'].apply(lambda x: get_embedding(embeddings_dir,x,layer=layer))\n",
    "        embedding_df.to_pickle(out_pkl)\n",
    "        del tmp_df\n",
    "\n",
    "if RUN_ESM:\n",
    "    harmonized_df = pd.read_csv(f'{HARMONIZE_DIR}/04-results/new_to_old_harmonized_dataset.csv').query('dataset != \"Mycocosm_Overlap\"').drop_duplicates(subset='AAseq',keep=False)\n",
    "    harmonized_df = harmonized_df[['dataset','AAseq','linear_harmonized_activity_scaled','linear_harmonized_activity']]\n",
    "\n",
    "    tmp_df = harmonized_df.copy()\n",
    "    tmp_df['ID'] = [f'harmonized-{str(i).zfill(5)}' for i in range(len(tmp_df))]\n",
    "    model = 'esm2_t33_650M_UR50D'\n",
    "    layer = 33\n",
    "\n",
    "    df_to_fasta(tmp_df,'ID','AAseq','../../02-OUTPUT/20-sandbox/harmonized.faa')\n",
    "    print('running ESM')\n",
    "    run_esm(f'../../02-OUTPUT/20-sandbox//harmonized.faa',\n",
    "            f'../../02-OUTPUT/20-sandbox/{model}',\n",
    "            model=model,\n",
    "            layer=layer)\n",
    "    print('saving representations')\n",
    "    embedding_df = tmp_df.copy()\n",
    "    embedding_df[model] = embedding_df['ID'].apply(lambda x: get_embedding(f'../../02-OUTPUT/20-sandbox/{model}/',x,layer=layer))\n",
    "    embedding_df.to_pickle(f'../../02-OUTPUT/20-sandbox/harmonized_{model}-layer{layer}-representations.pkl')\n",
    "    del tmp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# standardize datasets for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(f'{COMPARE_DIR}/morffy_dataset.pkl'):\n",
    "    datasets = {}\n",
    "    hummel_df = pd.read_pickle(f'{ENCODING_DIR}/01-dataset/hummel_encodings.pkl').drop_duplicates(subset='AAseq',keep=False)\n",
    "    onehot_X = np.asarray([np.array(emb) for emb in hummel_df['onehot_encoding']])\n",
    "    alphabet_X = np.asarray([[AA_TO_I[aa] for aa in x] for x in hummel_df['AAseq']])\n",
    "    hummel_df = pd.read_pickle(f'{EMBEDDING_DIR}/01-dataset/esm2_t33_650M_UR50D-layer33-representations.pkl').drop_duplicates(subset='AAseq',keep=False)\n",
    "    esm_X = np.asarray([np.array(emb) for emb in hummel_df['esm2_t33_650M_UR50D']])\n",
    "    y = hummel_df['activity'].to_numpy()\n",
    "    y_cont = y.reshape(-1, 1)\n",
    "    y_cont = preprocessing.MinMaxScaler().fit_transform(y_cont)\n",
    "    y_cont = preprocessing.StandardScaler().fit_transform(y_cont)\n",
    "    thresh = y_cont.mean() + y_cont.std() \n",
    "    y_bin = (y_cont >= thresh).astype(np.int64).reshape(-1, 1)\n",
    "    datasets['hummel'] = {'onehot':onehot_X, \n",
    "                          'onehot_flat':onehot_X.reshape(onehot_X.shape[0],onehot_X.shape[1]*onehot_X.shape[2]),\n",
    "                          'alphabet':alphabet_X, \n",
    "                          'esm':esm_X,\n",
    "                          'esm_mean':esm_X.mean(axis=-1),\n",
    "                          'y_cont':y_cont,\n",
    "                          'y_bin':y_bin}\n",
    "    with open(f'{COMPARE_DIR}/hummel_dataset.pkl', 'wb') as f:\n",
    "        pickle.dump(datasets, f)\n",
    "\n",
    "    datasets = {}\n",
    "    harmonized_df = pd.read_pickle(f'{HARMONIZE_DIR}/04-results/esm2_t33_650M_UR50D-layer33-representations.pkl').query('dataset != \"Mycocosm_Overlap\"').drop_duplicates(subset='AAseq',keep=False)\n",
    "    onehot_X = np.asarray([np.array(emb) for emb in harmonized_df['AAseq'].apply(lambda x: one_hot_encode(x))])\n",
    "    alphabet_X = np.asarray([[AA_TO_I[aa] for aa in x] for x in harmonized_df['AAseq']])\n",
    "    esm_X = np.asarray([np.array(emb) for emb in harmonized_df['esm2_t33_650M_UR50D']])\n",
    "    y = harmonized_df['linear_harmonized_activity'].to_numpy()\n",
    "    y_cont = y.reshape(-1, 1)\n",
    "    y_cont = preprocessing.MinMaxScaler().fit_transform(y_cont)\n",
    "    y_cont = preprocessing.StandardScaler().fit_transform(y_cont)\n",
    "    thresh = y_cont.mean() + y_cont.std() \n",
    "    y_bin = (y_cont >= thresh).astype(np.int64).reshape(-1, 1)\n",
    "\n",
    "    datasets['harmonized'] = {'onehot':onehot_X, \n",
    "                              'onehot_flat':onehot_X.reshape(onehot_X.shape[0],onehot_X.shape[1]*onehot_X.shape[2]),\n",
    "                              'alphabet':alphabet_X, \n",
    "                              'esm':esm_X,\n",
    "                              'esm_mean':esm_X.mean(axis=-1),\n",
    "                              'y_cont':y_cont,\n",
    "                              'y_bin':y_bin}\n",
    "    with open(f'{COMPARE_DIR}/harmonized_dataset.pkl', 'wb') as f:\n",
    "        pickle.dump(datasets, f)\n",
    "\n",
    "    datasets = {}\n",
    "    morffy_df = pd.read_csv(f'{MORFFY_DIR}/01-dataset/TrainingsData.csv').drop_duplicates(subset='Sequence',keep=False)\n",
    "    morffy_df = morffy_df[morffy_df['Sequence'].apply(lambda x: len(x) == 40)].rename(columns={'Sequence':'AAseq'})\n",
    "    onehot_X = np.asarray([np.array(emb) for emb in morffy_df['AAseq'].apply(lambda x: one_hot_encode(x))])\n",
    "    alphabet_X = np.asarray([[AA_TO_I[aa] for aa in x] for x in morffy_df['AAseq']])\n",
    "    morffy_df = pd.read_pickle(f'{MORFFY_DIR}/01-dataset/morffy_esm2_t33_650M_UR50D-layer33-representations.pkl').drop_duplicates(subset='AAseq',keep=False)\n",
    "    morffy_df = morffy_df[morffy_df['AAseq'].apply(lambda x: len(x) == 40)]\n",
    "    esm_X = np.asarray([np.array(emb) for emb in morffy_df['esm2_t33_650M_UR50D']])\n",
    "    y = morffy_df['Score'].to_numpy()\n",
    "    y_cont = y.reshape(-1, 1)\n",
    "    y_cont = preprocessing.MinMaxScaler().fit_transform(y_cont)\n",
    "    y_cont = preprocessing.StandardScaler().fit_transform(y_cont)\n",
    "    thresh = y_cont.mean() + y_cont.std() \n",
    "    y_bin = (y_cont >= thresh).astype(np.int64).reshape(-1, 1)\n",
    "    datasets['morffy'] = {'onehot':onehot_X,\n",
    "                          'onehot_flat':onehot_X.reshape(onehot_X.shape[0],onehot_X.shape[1]*onehot_X.shape[2]),\n",
    "                          'alphabet':alphabet_X, \n",
    "                          'esm':esm_X,\n",
    "                          'esm_mean':esm_X.mean(axis=-1),\n",
    "                          'y_cont':y_cont,\n",
    "                          'y_bin':y_bin}\n",
    "    with open(f'{COMPARE_DIR}/morffy_dataset.pkl', 'wb') as f:\n",
    "        pickle.dump(datasets, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# standardize held-out test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVE_IDXS:\n",
    "    for dataset_path in glob(f'{COMPARE_DIR}/*dataset.pkl'):\n",
    "        dataset = pd.read_pickle(dataset_path)\n",
    "        for dataset_name, data in dataset.items():\n",
    "            X, y_cont, y_bin = data['esm'], data['y_cont'], data['y_bin']\n",
    "            print(dataset_name)\n",
    "            for state in range(1,4):\n",
    "                threshold = np.median(y_cont)\n",
    "                _, idx = split_dataset(X,y_cont,threshold,scaler='standard',random_state=state)\n",
    "                with open(f'{ADHUNTER_DIR}/03-comparison_idxs/{dataset_name}_esm_dataset_idx_state{state}.pkl', 'wb') as f:\n",
    "                    pickle.dump(idx, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluate ADhunter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_df = pd.read_csv(f'{HAMMING_ENSEMBLE_DIR}/03-results/top_models.csv')\n",
    "df_filtered = pd.read_csv(f'{HAMMING_ENSEMBLE_DIR}/03-results/model_testing_filtered.csv').set_index('file')\n",
    "\n",
    "if RUN_MODELS:\n",
    "    for dataset_path in glob(f'{COMPARE_DIR}/*dataset.pkl'):\n",
    "        dataset = pd.read_pickle(dataset_path)\n",
    "        for dataset_name, data in dataset.items():\n",
    "            if dataset_name == 'hummel':\n",
    "                continue\n",
    "            X, y_cont, y_bin = data['esm'], data['y_cont'], data['y_bin']\n",
    "            print(dataset_name)\n",
    "            for state in range(1,4):\n",
    "                print(state)\n",
    "                threshold = np.median(y_cont)\n",
    "                dataset, idx = split_dataset(X,y_cont,threshold,scaler='standard',random_state=state)\n",
    "            \n",
    "                train, val, _ = dataset\n",
    "                (X_train, _, y_cont_train) = train\n",
    "                (X_val, _, y_cont_val) = val\n",
    "\n",
    "                out_dir = f'{ADHUNTER_DIR}/02-ensemble'\n",
    "                _, selected_indices = select_maximally_different_arrays(df_filtered.to_numpy(), 20)\n",
    "                hits_df = df_filtered.reset_index()\n",
    "                hits_df = hits_df.loc[selected_indices].set_index('file')\n",
    "                for idx, row in query_df[query_df.file.isin(hits_df.index)].reset_index(drop=True).iterrows():\n",
    "                    print(f'==================={idx/20}===================')\n",
    "                    train_ds = TensorDataset(X_train.to(torch.float), y_cont_train.to(torch.float))\n",
    "                    val_ds = TensorDataset(X_val.to(torch.float), y_cont_val.to(torch.float))\n",
    "                    train_dl = DataLoader(train_ds, batch_size=row['batch_size'], shuffle=False)\n",
    "                    val_dl = DataLoader(val_ds, batch_size=row['batch_size'], shuffle=False)\n",
    "\n",
    "                    model = ADhunterSystem_v2(\n",
    "                        embedding_size=X_train[0].shape[1],\n",
    "                        hidden=row['hidden_size'], \n",
    "                        kernel_size=row['kernel_size'], \n",
    "                        dilation=row['dilation'], \n",
    "                        num_res_blocks=row['num_res_blocks'],\n",
    "                        seq_len=X_train[0].shape[0]\n",
    "                        )\n",
    "\n",
    "                    OUTPUT_FILE = f\"{dataset_name}_ADhunter_v2_state{state}_h{row['hidden_size']}_k{row['kernel_size']}_d{row['dilation']}_r{row['num_res_blocks']}_b{row['batch_size']}\"\n",
    "                    csv_logger = CSVLogger(f\"{out_dir}/01-logs\",name=OUTPUT_FILE,version='')\n",
    "                    checkpoint_callback = ModelCheckpoint(dirpath=f\"{out_dir}/02-models\", monitor=\"val_loss\", filename=OUTPUT_FILE, save_last=False)\n",
    "                    early_stopping = EarlyStopping('val_loss', patience=PATIENCE)\n",
    "                    trainer = pl.Trainer(accelerator='gpu', devices=1, callbacks=[checkpoint_callback, early_stopping], logger=[csv_logger], max_epochs=MAX_EPOCHS)\n",
    "                    trainer.fit(model, train_dataloaders=train_dl, val_dataloaders=val_dl)\n",
    "\n",
    "                    model = model.load_from_checkpoint(checkpoint_callback.best_model_path)\n",
    "                    torch.save(model.cpu().state_dict(), f'{out_dir}/02-models/{OUTPUT_FILE}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_18eb7\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_18eb7_level0_col0\" class=\"col_heading level0 col0\" >r</th>\n",
       "      <th id=\"T_18eb7_level0_col1\" class=\"col_heading level0 col1\" >rmse</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >dataset</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_18eb7_level0_row0\" class=\"row_heading level0 row0\" >harmonized</th>\n",
       "      <td id=\"T_18eb7_row0_col0\" class=\"data row0 col0\" >0.817676</td>\n",
       "      <td id=\"T_18eb7_row0_col1\" class=\"data row0 col1\" >0.576275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_18eb7_level0_row1\" class=\"row_heading level0 row1\" >morffy</th>\n",
       "      <td id=\"T_18eb7_row1_col0\" class=\"data row1 col0\" >0.681990</td>\n",
       "      <td id=\"T_18eb7_row1_col1\" class=\"data row1 col1\" >0.739541</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fa2015ae5e0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if RUN_MODELS:\n",
    "    ensemble_tests = {}\n",
    "    ensemble_metrics = {}\n",
    "    for dataset_path in glob(f'{COMPARE_DIR}/*dataset.pkl'):\n",
    "        dataset = pd.read_pickle(dataset_path)\n",
    "        for dataset_name, data in dataset.items():\n",
    "            if dataset_name == 'hummel':\n",
    "                continue\n",
    "            X, y_cont, y_bin = data['esm'], data['y_cont'], data['y_bin']\n",
    "            print(dataset_name)\n",
    "            for state in range(1,4):\n",
    "                threshold = np.median(y_cont)\n",
    "                dataset, idx = split_dataset(X,y_cont,threshold,scaler='standard',random_state=state)\n",
    "                _, _, test = dataset\n",
    "                (X_test, y_bin_test, y_test) = test\n",
    "                y_bin_test = y_bin_test.reshape(-1).numpy()\n",
    "\n",
    "                predictions, variances = get_new_uncertainty(dataset_name, state, X_test)\n",
    "                ensemble_tests[f'{dataset_name}_{state}'] = [predictions, variances, y_bin_test, y_test.numpy().reshape(-1)]\n",
    "                ensemble_metrics[f'{dataset_name}_{state}'] = [\n",
    "                    pearsonr(y_test.numpy().reshape(-1),predictions),\n",
    "                    spearmanr(y_test.numpy().reshape(-1),predictions),\n",
    "                    mean_squared_error(y_test.numpy().reshape(-1),predictions,squared=False),\n",
    "                ]\n",
    "    ensemble_tests = pd.DataFrame(ensemble_tests).T.reset_index()\n",
    "    ensemble_tests.to_pickle(f'{ADHUNTER_DIR}/02-ensemble/03-results/ensemble_test.pkl')\n",
    "    ensemble_metrics = pd.DataFrame(ensemble_metrics).T.reset_index()\n",
    "    ensemble_metrics.to_pickle(f'{ADHUNTER_DIR}/02-ensemble/03-results/ensemble_metrics.pkl')\n",
    "else:\n",
    "    ensemble_tests = pd.read_pickle(f'{ADHUNTER_DIR}/02-ensemble/03-results/ensemble_test.pkl')\n",
    "    ensemble_metrics = pd.read_pickle(f'{ADHUNTER_DIR}/02-ensemble/03-results/ensemble_metrics.pkl')\n",
    "ensemble_metrics['state'] = ensemble_metrics['index'].apply(lambda x: x.split('_')[-1])\n",
    "ensemble_metrics['dataset'] = ensemble_metrics['index'].apply(lambda x: x.split('_')[0])\n",
    "ensemble_metrics['r'] = ensemble_metrics[0].apply(lambda x: x[0])\n",
    "ensemble_metrics['rmse'] = ensemble_metrics[2]\n",
    "ensemble_metrics.drop(columns=['index',0,1,2,'state']).groupby('dataset').mean().style\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_60581\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_60581_level0_col0\" class=\"col_heading level0 col0\" >Accuracy</th>\n",
       "      <th id=\"T_60581_level0_col1\" class=\"col_heading level0 col1\" >F1 Score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >dataset</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_60581_level0_row0\" class=\"row_heading level0 row0\" >harmonized</th>\n",
       "      <td id=\"T_60581_row0_col0\" class=\"data row0 col0\" >0.905041</td>\n",
       "      <td id=\"T_60581_row0_col1\" class=\"data row0 col1\" >0.909919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_60581_level0_row1\" class=\"row_heading level0 row1\" >morffy</th>\n",
       "      <td id=\"T_60581_row1_col0\" class=\"data row1 col0\" >0.931724</td>\n",
       "      <td id=\"T_60581_row1_col1\" class=\"data row1 col1\" >0.935101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fa1edc68670>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_tests.columns = ['params','predictions','uncertainty','y_bin_test','y_test']\n",
    "classification_metrics_dict = {}\n",
    "for idx,row in ensemble_tests.iterrows():\n",
    "    dataset = row['params']\n",
    "    y_test_hat = row['predictions']\n",
    "    y_test = row['y_test']\n",
    "    y_test_hat_bin = (y_test_hat >= 1.0).astype(int)\n",
    "    y_test_bin = (y_test >= 1.0).astype(int)\n",
    "    classification_metrics_dict[dataset] = classification_metrics(y_test_hat_bin,y_test_bin)\n",
    "\n",
    "classification_df = pd.DataFrame(classification_metrics_dict).T.reset_index()\n",
    "classification_df['state'] = classification_df['index'].apply(lambda x: x.split('_')[-1])\n",
    "classification_df['dataset'] = classification_df['index'].apply(lambda x: x.split('_')[0])\n",
    "classification_df.drop(columns=['index','state']).groupby('dataset').mean().style"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adhunter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
